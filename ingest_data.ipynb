{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "098c423d-8e12-4eba-a1b6-f99004c22ea9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: dotenv in /opt/app-root/lib64/python3.11/site-packages (from -r requirements.txt (line 1)) (0.9.9)\n",
      "Requirement already satisfied: llama_stack_client in /opt/app-root/lib64/python3.11/site-packages (from -r requirements.txt (line 2)) (0.2.12)\n",
      "Requirement already satisfied: pandas in /opt/app-root/lib64/python3.11/site-packages (from -r requirements.txt (line 3)) (2.2.3)\n",
      "Collecting fire (from -r requirements.txt (line 4))\n",
      "  Downloading fire-0.7.0.tar.gz (87 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: python-dotenv in /opt/app-root/lib64/python3.11/site-packages (from dotenv->-r requirements.txt (line 1)) (1.1.1)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /opt/app-root/lib64/python3.11/site-packages (from llama_stack_client->-r requirements.txt (line 2)) (4.9.0)\n",
      "Requirement already satisfied: click in /opt/app-root/lib64/python3.11/site-packages (from llama_stack_client->-r requirements.txt (line 2)) (8.1.8)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /opt/app-root/lib64/python3.11/site-packages (from llama_stack_client->-r requirements.txt (line 2)) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /opt/app-root/lib64/python3.11/site-packages (from llama_stack_client->-r requirements.txt (line 2)) (0.28.1)\n",
      "Requirement already satisfied: prompt-toolkit in /opt/app-root/lib64/python3.11/site-packages (from llama_stack_client->-r requirements.txt (line 2)) (3.0.50)\n",
      "Requirement already satisfied: pyaml in /opt/app-root/lib64/python3.11/site-packages (from llama_stack_client->-r requirements.txt (line 2)) (25.7.0)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in /opt/app-root/lib64/python3.11/site-packages (from llama_stack_client->-r requirements.txt (line 2)) (1.10.21)\n",
      "Requirement already satisfied: rich in /opt/app-root/lib64/python3.11/site-packages (from llama_stack_client->-r requirements.txt (line 2)) (13.9.4)\n",
      "Requirement already satisfied: sniffio in /opt/app-root/lib64/python3.11/site-packages (from llama_stack_client->-r requirements.txt (line 2)) (1.3.1)\n",
      "Requirement already satisfied: termcolor in /opt/app-root/lib64/python3.11/site-packages (from llama_stack_client->-r requirements.txt (line 2)) (2.3.0)\n",
      "Requirement already satisfied: tqdm in /opt/app-root/lib64/python3.11/site-packages (from llama_stack_client->-r requirements.txt (line 2)) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.7 in /opt/app-root/lib64/python3.11/site-packages (from llama_stack_client->-r requirements.txt (line 2)) (4.12.2)\n",
      "Requirement already satisfied: numpy>=1.23.2 in /opt/app-root/lib64/python3.11/site-packages (from pandas->-r requirements.txt (line 3)) (2.2.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/app-root/lib64/python3.11/site-packages (from pandas->-r requirements.txt (line 3)) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/app-root/lib64/python3.11/site-packages (from pandas->-r requirements.txt (line 3)) (2025.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/app-root/lib64/python3.11/site-packages (from pandas->-r requirements.txt (line 3)) (2025.1)\n",
      "Requirement already satisfied: idna>=2.8 in /opt/app-root/lib64/python3.11/site-packages (from anyio<5,>=3.5.0->llama_stack_client->-r requirements.txt (line 2)) (3.10)\n",
      "Requirement already satisfied: certifi in /opt/app-root/lib64/python3.11/site-packages (from httpx<1,>=0.23.0->llama_stack_client->-r requirements.txt (line 2)) (2025.1.31)\n",
      "Requirement already satisfied: httpcore==1.* in /opt/app-root/lib64/python3.11/site-packages (from httpx<1,>=0.23.0->llama_stack_client->-r requirements.txt (line 2)) (1.0.7)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /opt/app-root/lib64/python3.11/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->llama_stack_client->-r requirements.txt (line 2)) (0.14.0)\n",
      "Requirement already satisfied: six>=1.5 in /opt/app-root/lib64/python3.11/site-packages (from python-dateutil>=2.8.2->pandas->-r requirements.txt (line 3)) (1.17.0)\n",
      "Requirement already satisfied: wcwidth in /opt/app-root/lib64/python3.11/site-packages (from prompt-toolkit->llama_stack_client->-r requirements.txt (line 2)) (0.2.13)\n",
      "Requirement already satisfied: PyYAML in /opt/app-root/lib64/python3.11/site-packages (from pyaml->llama_stack_client->-r requirements.txt (line 2)) (6.0.2)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /opt/app-root/lib64/python3.11/site-packages (from rich->llama_stack_client->-r requirements.txt (line 2)) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/app-root/lib64/python3.11/site-packages (from rich->llama_stack_client->-r requirements.txt (line 2)) (2.19.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in /opt/app-root/lib64/python3.11/site-packages (from markdown-it-py>=2.2.0->rich->llama_stack_client->-r requirements.txt (line 2)) (0.1.2)\n",
      "Building wheels for collected packages: fire\n",
      "  Building wheel for fire (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for fire: filename=fire-0.7.0-py3-none-any.whl size=114299 sha256=8bda20acf38cfa1475126e3b11997900439af8dce7c42aaebe69bcd18c389f1e\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-r8cqgojh/wheels/46/54/24/1624fd5b8674eb1188623f7e8e17cdf7c0f6c24b609dfb8a89\n",
      "Successfully built fire\n",
      "Installing collected packages: fire\n",
      "Successfully installed fire-0.7.0\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3dba2543-c7d6-45fe-a99c-a5a0e4d8967f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from dotenv import load_dotenv\n",
    "from llama_stack_client import LlamaStackClient, RAGDocument\n",
    "import pandas as pd\n",
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3a0a6057-be45-48b0-8800-bff0772f488b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Connected to Llama Stack server\n"
     ]
    }
   ],
   "source": [
    "sys.path.append('..')\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "logger.setLevel(\"INFO\")\n",
    "\n",
    "# Initialize the Llama Stack client\n",
    "client = LlamaStackClient(\n",
    "    base_url=os.getenv(\"LLAMA_STACK_SERVER_URL\", \"http://llamastack-server.rh-proposal-ai.svc.cluster.local:8321\")\n",
    ")\n",
    "\n",
    "file_path = \"data/Commercial-Direct-LATAM-USD-Q3-2025-Subscriptions.csv\"\n",
    "vector_db_skus_id = \"skus_rh_vector_db\"\n",
    "vector_db_ocp_id = \"ocp_rh_vector_db\"\n",
    "\n",
    "logger.info(\"Connected to Llama Stack server\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1fd8d36e-78ea-424d-a905-584d4c9d2d2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Preparing documents from data/Commercial-Direct-LATAM-USD-Q3-2025-Subscriptions.csv...\n",
      "INFO:__main__:Prepared 680 documents.\n"
     ]
    }
   ],
   "source": [
    "logger.info(f\"Preparing documents from {file_path}...\")\n",
    "\n",
    "# Read the CSV file into a DataFrame\n",
    "df = pd.read_csv(file_path)\n",
    "df.fillna('', inplace=True)\n",
    "\n",
    "# Get the list of columns in the DataFrame\n",
    "documents = []\n",
    "for index, row in df.iterrows():\n",
    "    # Combine relevant columns into the document's content/text\n",
    "    # This is what the embedding model will primarily \"read\"\n",
    "    text_content = (\n",
    "        f\"Product Name: {row['Product']}. SKU Description: {row['SKU_Description']}.\"\n",
    "        f\"SKU Number: {row['SKU']}. Price: ${row['List_Price']}.\"\n",
    "    )\n",
    "\n",
    "    # Include all original CSV columns as metadata\n",
    "    # This metadata can be used for filtering during retrieval or just for context\n",
    "    metadata = row.to_dict()\n",
    "\n",
    "    logger.debug(f\"Processing document {index + 1}: {text_content}...\")\n",
    "\n",
    "    # Create document object to ingest\n",
    "    documents.append(\n",
    "        RAGDocument(\n",
    "            # Use the index or a unique identifier\n",
    "            document_id=str(index) + \"-SKU-RH-LATAM-Q3-2025\",\n",
    "            # Assuming the content is plain text\n",
    "            mime_type=\"text/plain\",\n",
    "            # 'content' is the field for the main text\n",
    "            content=text_content,\n",
    "            metadata=metadata\n",
    "        )\n",
    "    )\n",
    "\n",
    "logger.info(f\"Prepared {len(documents)} documents.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "146edca2-cfef-47c1-b305-bd88404d24df",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Preparing documents from URLs...\n",
      "INFO:__main__:Prepared 1 documents from URLs.\n"
     ]
    }
   ],
   "source": [
    "logger.info(\"Preparing documents from URLs...\")\n",
    "\n",
    "# ingest the documents into the newly created document collection\n",
    "urls = [\n",
    "    #(\"https://raw.githubusercontent.com/lcoronad/ai-transformers/main/skus_red_hat_openshift.rst\", \"text/plain\"),\n",
    "    (\"https://www.openshift.guide/openshift-guide-screen.pdf\", \"application/pdf\"),\n",
    "]\n",
    "\n",
    "# Create document object to ingest\n",
    "documents_ocp = [\n",
    "    RAGDocument(\n",
    "        document_id=f\"num-{i}\",\n",
    "        content=url,\n",
    "        mime_type=url_type,\n",
    "        metadata={},\n",
    "    )\n",
    "    for i, (url, url_type) in enumerate(urls)\n",
    "]\n",
    "\n",
    "logger.info(f\"Prepared {len(documents_ocp)} documents from URLs.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c63dfc59-4203-45bd-97dd-fef3fd9676bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://llamastack-server.rh-proposal-ai.svc.cluster.local:8321/v1/vector-dbs \"HTTP/1.1 200 OK\"\n",
      "INFO:llama_stack_client._base_client:Retrying request to /v1/tool-runtime/rag-tool/insert in 0.466048 seconds\n",
      "INFO:httpx:HTTP Request: POST http://llamastack-server.rh-proposal-ai.svc.cluster.local:8321/v1/tool-runtime/rag-tool/insert \"HTTP/1.1 200 OK\"\n",
      "INFO:__main__:Documents ingested into RAG skus_rh_vector_db successfully.\n"
     ]
    }
   ],
   "source": [
    "# define and register the document collection to be used\n",
    "client.vector_dbs.register(\n",
    "    vector_db_id=vector_db_skus_id,\n",
    "    embedding_model=os.getenv(\"VDB_EMBEDDING\", \"all-MiniLM-L6-v2\"),\n",
    "    embedding_dimension=int(os.getenv(\"VDB_EMBEDDING_DIMENSION\", 384)),\n",
    "    provider_id=os.getenv(\"VDB_PROVIDER\", \"faiss\"),\n",
    ")\n",
    "\n",
    "# Insert documents to the vector database\n",
    "client.tool_runtime.rag_tool.insert(\n",
    "    documents=documents,\n",
    "    vector_db_id=vector_db_skus_id,\n",
    "    chunk_size_in_tokens=int(os.getenv(\"VECTOR_DB_CHUNK_SIZE\", 512)),\n",
    ")\n",
    "\n",
    "logger.info(f\"Documents ingested into RAG {vector_db_skus_id} successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0b17e34c-1b71-40f1-ad0d-c39eec1e125d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://llamastack-server.rh-proposal-ai.svc.cluster.local:8321/v1/vector-dbs \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST http://llamastack-server.rh-proposal-ai.svc.cluster.local:8321/v1/tool-runtime/rag-tool/insert \"HTTP/1.1 200 OK\"\n",
      "INFO:__main__:Documents ingested into RAG ocp_rh_vector_db successfully.\n"
     ]
    }
   ],
   "source": [
    "# define and register the document collection to be used\n",
    "client.vector_dbs.register(\n",
    "    vector_db_id=vector_db_ocp_id,\n",
    "    embedding_model=os.getenv(\"VDB_EMBEDDING\", \"all-MiniLM-L6-v2\"),\n",
    "    embedding_dimension=int(os.getenv(\"VDB_EMBEDDING_DIMENSION\", 384)),\n",
    "    provider_id=os.getenv(\"VDB_PROVIDER\", \"faiss\"),\n",
    ")\n",
    "\n",
    "# Insert documents to the vector database\n",
    "client.tool_runtime.rag_tool.insert(\n",
    "    documents=documents_ocp,\n",
    "    vector_db_id=vector_db_ocp_id,\n",
    "    chunk_size_in_tokens=int(os.getenv(\"VECTOR_DB_CHUNK_SIZE\", 512)),\n",
    ")\n",
    "\n",
    "logger.info(f\"Documents ingested into RAG {vector_db_ocp_id} successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0d3863f7-339d-4d4a-8965-00f2ad59f63e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://llamastack-server.rh-proposal-ai.svc.cluster.local:8321/v1/tool-runtime/rag-tool/query \"HTTP/1.1 200 OK\"\n",
      "INFO:__main__:RAG Query from skus_rh_vector_db - Result: \n",
      "[TextContentItem(text='knowledge_search tool found 5 chunks:\\nBEGIN of knowledge_search tool results.\\n', type='text'), TextContentItem(text=\"Result 1\\nContent: Product Name: OpenShift AI. SKU Description: Red Hat OpenShift AI, Standard (2 Cores or 4 vCPUs).SKU Number: MCT4547. Price: $1.485,00.\\nMetadata: {'YEAR': 2025, 'QUARTER': 'Q3', 'SKU': 'MCT4547', 'SKU_Description': 'Red Hat OpenShift AI, Standard (2 Cores or 4 vCPUs)', 'Product': 'OpenShift AI', 'Currency': 'USD', 'List_Price': '1.485,00', 'Unit_of_Measure': 'CORE BAND', 'Cores': '2', 'Nodes': '0', 'Sockets': 0, 'Virtual_Guests': '0', 'Support_Level': 'L1-L3', 'Support_Type': 'Standard', 'Category': 'SUBSCRIPTIONS - ELS', 'Region': 'LATAM', 'Country': 'ALL', 'Service_Term': '1 YEARS', 'document_id': '307-SKU-RH-LATAM-Q3-2025'}\\n\", type='text'), TextContentItem(text=\"Result 2\\nContent: Product Name: OpenShift AI. SKU Description: Red Hat OpenShift AI, Standard (2 Cores or 4 vCPUs).SKU Number: MCT4547. Price: $1.485,00.\\nMetadata: {'YEAR': 2025, 'QUARTER': 'Q3', 'SKU': 'MCT4547', 'SKU_Description': 'Red Hat OpenShift AI, Standard (2 Cores or 4 vCPUs)', 'Product': 'OpenShift AI', 'Currency': 'USD', 'List_Price': '1.485,00', 'Unit_of_Measure': 'CORE BAND', 'Cores': '2', 'Nodes': '0', 'Sockets': 0, 'Virtual_Guests': '0', 'Support_Level': 'L1-L3', 'Support_Type': 'Standard', 'Category': 'SUBSCRIPTIONS - ELS', 'Region': 'LATAM', 'Country': 'ALL', 'Service_Term': '1 YEARS', 'document_id': '307-SKU-RH-LATAM-Q3-2025'}\\n\", type='text'), TextContentItem(text=\"Result 3\\nContent: Product Name: OpenShift AI. SKU Description: Red Hat AI Accelerator, Standard (1 Accelerator).SKU Number: MCT4722. Price: $743,00.\\nMetadata: {'YEAR': 2025, 'QUARTER': 'Q3', 'SKU': 'MCT4722', 'SKU_Description': 'Red Hat AI Accelerator, Standard (1 Accelerator)', 'Product': 'OpenShift AI', 'Currency': 'USD', 'List_Price': '743,00', 'Unit_of_Measure': 'AI ACCELERATOR', 'Cores': '0', 'Nodes': '0', 'Sockets': 0, 'Virtual_Guests': '0', 'Support_Level': 'L1-L3', 'Support_Type': 'Standard', 'Category': 'SUBSCRIPTIONS', 'Region': 'LATAM', 'Country': 'ALL', 'Service_Term': '1 YEARS', 'document_id': '309-SKU-RH-LATAM-Q3-2025'}\\n\", type='text'), TextContentItem(text=\"Result 4\\nContent: Product Name: OpenShift AI. SKU Description: Red Hat AI Accelerator, Standard (1 Accelerator).SKU Number: MCT4722. Price: $743,00.\\nMetadata: {'YEAR': 2025, 'QUARTER': 'Q3', 'SKU': 'MCT4722', 'SKU_Description': 'Red Hat AI Accelerator, Standard (1 Accelerator)', 'Product': 'OpenShift AI', 'Currency': 'USD', 'List_Price': '743,00', 'Unit_of_Measure': 'AI ACCELERATOR', 'Cores': '0', 'Nodes': '0', 'Sockets': 0, 'Virtual_Guests': '0', 'Support_Level': 'L1-L3', 'Support_Type': 'Standard', 'Category': 'SUBSCRIPTIONS', 'Region': 'LATAM', 'Country': 'ALL', 'Service_Term': '1 YEARS', 'document_id': '309-SKU-RH-LATAM-Q3-2025'}\\n\", type='text'), TextContentItem(text=\"Result 5\\nContent: Product Name: OpenShift AI. SKU Description: Red Hat OpenShift AI (Bare Metal Node), Standard (1-2 sockets up to 128 cores).SKU Number: MCT4797. Price: $13.200,00.\\nMetadata: {'YEAR': 2025, 'QUARTER': 'Q3', 'SKU': 'MCT4797', 'SKU_Description': 'Red Hat OpenShift AI (Bare Metal Node), Standard (1-2 sockets up to 128 cores)', 'Product': 'OpenShift AI', 'Currency': 'USD', 'List_Price': '13.200,00', 'Unit_of_Measure': 'PHYSICAL NODE', 'Cores': '0', 'Nodes': '0', 'Sockets': 2, 'Virtual_Guests': '0', 'Support_Level': 'L1-L3', 'Support_Type': 'Standard', 'Category': 'SUBSCRIPTIONS', 'Region': 'LATAM', 'Country': 'ALL', 'Service_Term': '1 YEARS', 'document_id': '311-SKU-RH-LATAM-Q3-2025'}\\n\", type='text'), TextContentItem(text='END of knowledge_search tool results.\\n', type='text'), TextContentItem(text='The above results were retrieved to help answer the user\\'s query: \"List of Red Hat OpenShift SKUs\". Use them as supporting information only in answering this query.\\n', type='text')]\n"
     ]
    }
   ],
   "source": [
    "query = \"List of Red Hat OpenShift SKUs\"\n",
    "\n",
    "# Execute the query against the vector database\n",
    "result = client.tool_runtime.rag_tool.query(\n",
    "    vector_db_ids=[vector_db_skus_id],\n",
    "    query_config={\"query\": query},\n",
    "    content=query,\n",
    ")\n",
    "\n",
    "logger.info(f\"RAG Query from {vector_db_skus_id} - Result: \\n{result.content}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "071d177b-2daf-4b90-9f2a-6e14ee31e745",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://llamastack-server.rh-proposal-ai.svc.cluster.local:8321/v1/tool-runtime/rag-tool/query \"HTTP/1.1 200 OK\"\n",
      "INFO:__main__:RAG Query from ocp_rh_vector_db - Result: \n",
      "[TextContentItem(text='knowledge_search tool found 5 chunks:\\nBEGIN of knowledge_search tool results.\\n', type='text'), TextContentItem(text='Result 1\\nContent: .\\nThese characteristics set OpenShift apart as an excellent Kubernetes platform for enterprise users.\\nThe latest version of OpenShift available at the time of this writing is 4.12.\\n3.2. Is Red Hat OpenShift Open Source?\\nRed Hat OpenShift is a commercial product based on an open-source project called OKD. This\\nacronym means \" OpenShift Kubernetes Distribution\" and is publicly available for everyone to\\ninspect and contribute. Like the upstream Kubernetes project, OKD developers use the Go\\nprogramming language.\\n3.3. How can I run OpenShift?\\nToday, Red Hat OpenShift is available through various mechanisms and formats:\\n• DevOps teams can install it in their data centers \"on-premise.\"\\n• Major hyperscalers such as AWS, Azure, Google Cloud Platform, and IBM Cloud offer managed\\nRed Hat OpenShift installations.\\n• Developers can either run OpenShift locally on their workstations using Red Hat OpenShift\\nLocal, also known as CRC or \"Code-Ready Containers\"\\n• They can also request a 30-day trial OpenShift cluster, offered by Red Hat, at no charge, for\\ntesting and evaluation purposes.\\nRed Hat OpenShift is an integrated Platform-as-a-Service for enterprise users based on Kubernetes.\\nIt is tightly integrated with advanced security settings, developer tooling, and monitoring\\nmechanisms, allowing DevOps teams to be more productive.\\n8\\nChapter 4. OpenShift-only Custom Resource\\nDefinitions\\nRed Hat OpenShift is a complete DevOps platform extending Kubernetes in various ways. It bundles\\na constellation of Custom Resource Definitions (CRDs) to make the life of developers and cluster\\nadministrators easier.\\nLet us talk first about the CRDs only available on OpenShift.\\n4.1. Project\\nAn OpenShift Project is similar to a Kubernetes namespace, but more tightly integrated into the\\nsecurity system of OpenShift through additional annotations.\\napiVersion: project.openshift.io/v1\\nkind: Project\\nmetadata:\\n\\xa0 name: linkedin-learning-project\\n\\xa0 annotations:\\n\\xa0   openshift.io/description: \"Project description\"\\n\\xa0   openshift.io/display-name: \"Display name\"\\n4.2. Route\\nThe OpenShift Route object was one of the primary inspirations during the development of the\\nIngress object. In OpenShift, Ingress and Route objects work together to ensure your applications\\nare available outside the cluster.\\napiVersion: route.openshift.io/v1\\nkind: Route\\nmetadata:\\n\\xa0 name: my-route\\nspec:\\n\\xa0 host:\\nMetadata: {\\'document_id\\': \\'num-0\\'}\\n', type='text'), TextContentItem(text=\"Result 2\\nContent:  the Developer Sandbox for Red Hat OpenShift, a free\\nservice by Red Hat with which developers can get access to a fully managed cluster for 30 days. The\\nDeveloper Sandbox for Red Hat OpenShift is an ideal solution for those interested in knowing more\\nabout OpenShift with the least effort.\\nWith the Developer Sandbox for Red Hat OpenShift, developers can deploy their applications in a\\nsecure and fully managed environment. Red Hat even provides guided activities, but developers\\ncan deploy their container images and use their deployment manifestos.\\nYou only need a free Red Hat developer account to use the Developer Sandbox for Red Hat\\nOpenShift. Your free Red Hat developer account will give you access to many resources and\\nsoftware, freely available at developers.redhat.com.\\nFigure 2. Red Hat Developer Sandbox home page\\n6.1. Demo\\nGo to developers.redhat.com/developer-sandbox and click on the [\\u2009Start your sandbox for free\\u2009] \\nbutton. Log into your account, and if this is the first time you have accessed the Developer Sandbox,\\nRed Hat will ask you to confirm your account. Confirmation is required to avoid service abuse. The\\neasiest way to verify your account is by entering a code sent via SMS. After entering the code, you’ll\\nreceive a confirmation email. Depending on the system’s load, this verification can take some time.\\nOnce confirmed, you can navigate with your browser to developers.redhat.com/developer-sandbox\\n13\\nand log in directly to your very own temporary OpenShift cluster! It’s that simple.\\n6.2. Dev Spaces\\nAn exciting feature of the Developer Sandbox for Red Hat OpenShift is Red Hat OpenShift Dev\\nSpaces, a fully integrated developer environment within Red Hat OpenShift, allowing developers to\\ncreate and deploy applications in various programming languages and frameworks from within the\\nOpenShift cluster.\\nRed Hat OpenShift Dev Spaces also includes the web version of Microsoft Visual Studio Code, the\\npopular text editor for developers. Visual Studio Code allows developers to work in a professional\\nenvironment, including Git support, deploying their applications in a few clicks on a ready-to-use\\nOpenShift cluster.\\nYou can create Red Hat OpenShift Dev Spaces workspaces in various technologies: Go, .NET, Java\\n(using the Quarkus framework), Node.js, Python, (©), C++, Rust, PHP, and Scala, among many others.\\nDev Spaces also integrates with the odo command line tool through an auto-generated Devfile,\\nspecifying all the dependencies and steps\\nMetadata: {'document_id': 'num-0'}\\n\", type='text'), TextContentItem(text=\"Result 3\\nContent: author are affiliated with Red Hat, Inc. or any of its subsidiaries in any way.\\nOpenShift®, Red Hat®, and the Red Hat logo are trademarks or registered trademarks of Red Hat,\\nInc. or its subsidiaries in the United States and other countries. All other trademarks are the\\nproperty of their respective owners.\\nToolchain\\nThis website and associated content in PDF, EPUB, and man page formats have been created using\\nthe following tools:\\n• Antora\\n• Asciidoctor\\n• Search engine powered by TNTSearch and SQLite\\n• Website built and deployed from a GitLab CI/CD pipeline.\\n47\\nIndex\\n@\\n.NET, 14, 24\\nA\\nAdministration perspective, 40\\nArgo CD, 28\\nASP.NET, 44\\nAWS, 8\\nAWS Lambda, 34\\nAzure, 8\\nAzure Functions, 34\\nB\\nBuildConfig, 10\\nC\\nC#, 44\\nC++, 14\\nCI/CD, 28\\nCode-Ready Containers, 8\\nCodeReady Containers, 15\\ncommand-line tools, 11\\ncontainer, 22\\ncontainer image, 19\\ncontainer registry, 24\\nCRC, 8, 15\\nCustom Resource Definitions, 9\\nD\\nDeploymentConfig, 9\\nDeveloper Catalog, 22\\nDeveloper perspective, 22, 25\\nDeveloper Sandbox for Red Hat OpenShift, 13\\nE\\nElasticSearch, 40\\nEnvoy, 37\\nF\\nFlux, 28\\nG\\nGit, 25\\nGit repository, 22\\nGitHub, 4\\nGo, 14, 24\\nGoogle, 34\\nGoogle Cloud Platform, 8\\nGrafana, 45\\nH\\nHorizontal scaling, 42\\nI\\nIBM Cloud, 8\\nImporting YAML, 23\\ninstall OpenShift Local, 16\\nJ\\nJAR file, 23\\nJava, 14, 24, 44\\nJavaScript, 24, 44\\nJenkins, 28\\nK\\nKiali, 36\\nKibana, 40, 40\\nKnative, 34\\nKubernetes, 7\\nL\\nlogs, 40\\nM\\nMicroservices, 36\\nmonitor, 40\\nN\\nNode.js, 14\\nnon-root accounts, 20\\nO\\nOpenShift 4.12, 33\\nOpenShift Kubernetes Distribution, 8\\nMetadata: {'document_id': 'num-0'}\\n\", type='text'), TextContentItem(text=\"Result 4\\nContent:  over 28 years of experience, currently working as\\nSenior Architect for Red Hat. He is a published author, trainer, and speaker. He has written many\\nbooks about software development and has shipped cloud, mobile, and desktop apps since 1996.\\nAdrian holds a Master in Information Technology from the University of Liverpool.\\nAdrian is the co-creator of De Programmatica Ipsum , a monthly publication about the impact of\\nsoftware in society.\\nWhen not coding or teaching, Adrian likes to spend time with his wife Claudia, his cat Max and his\\nOlivetti Lettera 22 typewriter.\\n1\\nChapter 1. Course Requirements\\nThis section enumerates some knowledge and tools that would be useful to follow this course about\\nRed Hat OpenShift.\\n1.1. Audience\\nThis course is geared towards developers interested in running applications in OpenShift. It is also\\nuseful for DevOps engineers who want to learn how to use OpenShift to build and deploy cloud\\nnative applications.\\n1.2. Recommended Tools\\nHere’s some basic tooling to try Red Hat OpenShift by yourself:\\n• Operating System\\n◦ Microsoft Windows, Apple macOS, or Linux.\\n• Applications\\n◦ Git\\n◦ Podman, Podman Desktop, Docker, or Docker Desktop\\n• Accounts\\n◦ A Red Hat Developer account\\n◦ A GitHub account\\n1.3. Knowledge\\nYou should be familiar with:\\n• Containers\\n◦ Dockerfile and Containerfile\\n◦ Docker or Podman\\n◦ Container registries\\n• Kubernetes\\n◦ Deploying applications\\n◦ Manifests in YAML format\\n◦ Performing basic administration tasks\\n• Git\\n◦ GitHub and GitLab\\n2\\n1.4. Example Code\\nThis course uses 4 simple, open-source containerized applications:\\n• simple-go-api, written in the Go programming language\\n◦ Container: registry.gitlab.com/akosma/simple-go-api:latest\\n• catalog-dotnet, written in C# with the ASP.NET framework.\\n◦ Container: registry.gitlab.com/akosma/catalog-dotnet:latest\\n• image-go-api, written in the Go programming language , using the Gin Web Framework  and\\ngenerating images via ImageMagick.\\n◦ Container: registry.gitlab.com/akosma/image-go-api:latest\\n• simple-deno-api, written in TypeScript and running with Oak under the Deno runtime\\n◦ Container: registry.gitlab.com/akosma\\nMetadata: {'document_id': 'num-0'}\\n\", type='text'), TextContentItem(text=\"Result 5\\nContent:  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . \\xa04\\nGetting Started. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . \\xa06\\n3. What is OpenShift? . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . \\xa07\\n3.1. What is Red Hat OpenShift?. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . \\xa07\\n3.2. Is Red Hat OpenShift Open Source?. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . \\xa08\\n3.3. How can I run OpenShift? . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . \\xa08\\n4. OpenShift-only Custom Resource Definitions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . \\xa09\\n4.1. Project . . . . . . . . . . .\\nMetadata: {'document_id': 'num-0'}\\n\", type='text'), TextContentItem(text='END of knowledge_search tool results.\\n', type='text'), TextContentItem(text='The above results were retrieved to help answer the user\\'s query: \"What is Red Hat OpenShift?\". Use them as supporting information only in answering this query.\\n', type='text')]\n"
     ]
    }
   ],
   "source": [
    "query = \"What is Red Hat OpenShift?\"\n",
    "\n",
    "# Execute the query against the vector database\n",
    "result = client.tool_runtime.rag_tool.query(\n",
    "    vector_db_ids=[vector_db_ocp_id],\n",
    "    query_config={\"query\": query},\n",
    "    content=query,\n",
    ")\n",
    "\n",
    "logger.info(f\"RAG Query from {vector_db_ocp_id} - Result: \\n{result.content}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4fb4e25-19b1-431a-bb1c-d8aa19eeac47",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
